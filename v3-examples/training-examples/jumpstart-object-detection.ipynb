{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker JumpStart Object Detection for Bird Species\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Object detection is the process of identifying and localizing objects in an image. A typical object detection solution takes an image as input and provides a bounding box on the image where an object of interest is found. It also identifies what type of object the box encapsulates.\n",
    "\n",
    "This notebook is an end-to-end example showing how Amazon SageMaker JumpStart can be used to train an object detection model on a custom dataset. We use the [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset, which contains images of 200 bird species with bounding box annotations.\n",
    "\n",
    "### What is JumpStart?\n",
    "\n",
    "JumpStart provides pre-trained models that can be fine-tuned on your custom data without writing training scripts. This notebook uses the **Faster R-CNN with ResNet-50** backbone, which is a popular object detection architecture that balances accuracy and speed.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- How to prepare custom data in COCO format for object detection\n",
    "- How to train a JumpStart model on your own dataset\n",
    "- How to deploy and test an object detection model\n",
    "- How to visualize detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "from sagemaker.train.model_trainer import ModelTrainer\n",
    "from sagemaker.train.configs import InputData\n",
    "from sagemaker.core.jumpstart.configs import JumpStartConfig\n",
    "\n",
    "sagemaker_session = Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = get_execution_role()\n",
    "\n",
    "print(f'Bucket: {bucket}')\n",
    "print(f'Role: {role}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and Prepare CUB Dataset\n",
    "\n",
    "The [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset contains 11,788 images across 200 bird species. Each species comes with around 60 images, with a typical size of about 350 pixels by 500 pixels. Bounding boxes are provided for each bird in the image.\n",
    "\n",
    "For this demonstration, we'll use a subset of 5 bird species to keep training time manageable. The same approach works for all 200 species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CUB-200-2011 dataset\n",
    "!wget -q https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz\n",
    "!tar -xzf CUB_200_2011.tgz\n",
    "print('Dataset downloaded and extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create COCO Format Annotations\n",
    "\n",
    "JumpStart object detection models expect data in COCO (Common Objects in Context) format, which is a standard format for object detection datasets.\n",
    "\n",
    "### COCO Format Structure\n",
    "\n",
    "A COCO dataset consists of:\n",
    "- **images**: List of image metadata (id, filename, width, height)\n",
    "- **annotations**: List of bounding boxes with category labels\n",
    "- **categories**: List of object categories (id, name)\n",
    "\n",
    "### Important Format Requirements\n",
    "\n",
    "1. **Category IDs start at 1**: COCO reserves category 0 for background, so your object categories should be 1, 2, 3, etc.\n",
    "2. **Bounding box format**: Use corner coordinates `[x_min, y_min, x_max, y_max]`\n",
    "3. **Coordinate validation**: Ensure all bounding boxes are within image bounds and have positive dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def create_coco_dataset(base_dir, num_classes=5):\n",
    "    \"\"\"Create COCO format annotations with 1-indexed categories.\"\"\"\n",
    "    images_dict = {}\n",
    "    with open(os.path.join(base_dir, 'images.txt')) as f:\n",
    "        for line in f:\n",
    "            img_id, img_path = line.strip().split(' ', 1)\n",
    "            images_dict[img_id] = os.path.basename(img_path)\n",
    "    \n",
    "    bboxes_dict = {}\n",
    "    with open(os.path.join(base_dir, 'bounding_boxes.txt')) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            img_id = parts[0]\n",
    "            x, y, w, h = map(float, parts[1:5])\n",
    "            bboxes_dict[img_id] = [x, y, w, h]\n",
    "    \n",
    "    labels_dict = {}\n",
    "    with open(os.path.join(base_dir, 'image_class_labels.txt')) as f:\n",
    "        for line in f:\n",
    "            img_id, class_id = line.strip().split()\n",
    "            labels_dict[img_id] = int(class_id)  # Keep 1-indexed\n",
    "    \n",
    "    split_dict = {}\n",
    "    with open(os.path.join(base_dir, 'train_test_split.txt')) as f:\n",
    "        for line in f:\n",
    "            img_id, is_train = line.strip().split()\n",
    "            split_dict[img_id] = int(is_train) == 1\n",
    "    \n",
    "    valid_classes = list(range(1, num_classes + 1))  # 1-indexed: [1, 2, 3, 4, 5]\n",
    "    \n",
    "    # Combine train and val into single dataset\n",
    "    coco = {\n",
    "        'images': [],\n",
    "        'annotations': [],\n",
    "        'categories': [{'id': i, 'name': f'bird_class_{i}'} for i in valid_classes]\n",
    "    }\n",
    "    \n",
    "    ann_id = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for img_id in sorted(images_dict.keys()):\n",
    "        class_id = labels_dict[img_id]\n",
    "        if class_id not in valid_classes:\n",
    "            continue\n",
    "        \n",
    "        # Get image dimensions\n",
    "        img_path_full = None\n",
    "        with open(os.path.join(base_dir, 'images.txt')) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(img_id + ' '):\n",
    "                    img_path_full = line.strip().split(' ', 1)[1]\n",
    "                    break\n",
    "        \n",
    "        img_full_path = os.path.join(base_dir, 'images', img_path_full)\n",
    "        img = Image.open(img_full_path)\n",
    "        width, height = img.size\n",
    "        \n",
    "        x, y, w, h = bboxes_dict[img_id]\n",
    "        \n",
    "        # Fix negative dimensions\n",
    "        if w < 0:\n",
    "            x = x + w\n",
    "            w = abs(w)\n",
    "        if h < 0:\n",
    "            y = y + h\n",
    "            h = abs(h)\n",
    "        \n",
    "        # Clamp to image bounds\n",
    "        x = max(0, x)\n",
    "        y = max(0, y)\n",
    "        w = min(w, width - x)\n",
    "        h = min(h, height - y)\n",
    "        \n",
    "        # Skip invalid boxes\n",
    "        if w <= 0 or h <= 0:\n",
    "            print(f'Skipping image {img_id} with invalid bbox: [{x}, {y}, {w}, {h}]')\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Add image and annotation\n",
    "        coco['images'].append({\n",
    "            'id': int(img_id),\n",
    "            'file_name': images_dict[img_id],\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "        \n",
    "        coco['annotations'].append({\n",
    "            'id': ann_id,\n",
    "            'image_id': int(img_id),\n",
    "            'category_id': class_id,\n",
    "            'bbox': [x, y, x + w, y + h],  # Convert to [x_min, y_min, x_max, y_max] for PyTorch\n",
    "            'area': w * h,\n",
    "            'iscrowd': 0\n",
    "        })\n",
    "        ann_id += 1\n",
    "    \n",
    "    os.makedirs('annotations', exist_ok=True)\n",
    "    with open('annotations/combined.json', 'w') as f:\n",
    "        json.dump(coco, f)\n",
    "    \n",
    "    print(f'Total: {len(coco[\"images\"])} images, {len(coco[\"annotations\"])} annotations, {skipped} skipped')\n",
    "    return coco\n",
    "\n",
    "coco_data = create_coco_dataset('CUB_200_2011', num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Flat Image Structure\n",
    "\n",
    "The CUB dataset organizes images in nested folders by species (e.g., `001.Black_footed_Albatross/image1.jpg`). However, JumpStart expects all images in a single flat directory.\n",
    "\n",
    "We'll copy all images to a flat directory structure where each image filename is unique. The COCO annotations file will reference these filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create flat image directory\n",
    "if os.path.exists('flat_images'):\n",
    "    shutil.rmtree('flat_images')\n",
    "os.makedirs('flat_images', exist_ok=True)\n",
    "\n",
    "# Copy all images to flat directory\n",
    "print('Creating flat image structure...')\n",
    "for root, dirs, files in os.walk('CUB_200_2011/images'):\n",
    "    for file in files:\n",
    "        if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            src = os.path.join(root, file)\n",
    "            dst = os.path.join('flat_images', file)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "print(f'Copied {len(os.listdir(\"flat_images\"))} images to flat directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload to S3\n",
    "\n",
    "SageMaker training jobs read data from S3. We need to upload our prepared dataset in the following structure:\n",
    "\n",
    "```\n",
    "s3://bucket/prefix/train/\n",
    "├── images/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "└── annotations.json\n",
    "```\n",
    "\n",
    "**Important**: JumpStart expects all data in a single `training` channel. The training script will automatically split it 80% for training and 20% for validation. Do not create separate train and validation folders.\n",
    "\n",
    "We use a timestamped prefix to ensure we're using fresh data and not cached versions from previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use timestamped prefix to avoid caching\n",
    "prefix = f'jumpstart-od-birds-{int(time.time())}'\n",
    "train_s3 = f's3://{bucket}/{prefix}/train'\n",
    "\n",
    "print(f'Uploading to: {train_s3}')\n",
    "\n",
    "# Upload images\n",
    "!aws s3 sync flat_images {train_s3}/images/ --quiet\n",
    "\n",
    "# Upload combined annotations\n",
    "!aws s3 cp annotations/combined.json {train_s3}/annotations.json\n",
    "\n",
    "print('Upload complete!')\n",
    "print(f'\\nData location: {train_s3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Upload\n",
    "\n",
    "Before starting a training job, it's good practice to verify that:\n",
    "1. Data was uploaded successfully to S3\n",
    "2. The annotations file is valid JSON\n",
    "3. The number of images and annotations match expectations\n",
    "\n",
    "This helps catch issues early before spending time and money on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify S3 data\n",
    "!aws s3 ls {train_s3}/\n",
    "\n",
    "# Check annotation count\n",
    "result = !aws s3 cp {train_s3}/annotations.json - | python3 -c \"import json, sys; d=json.load(sys.stdin); print(f'Images: {len(d[\\\"images\\\"])}, Annotations: {len(d[\\\"annotations\\\"])}')\"\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Now we're ready to train our object detection model using JumpStart.\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "We use `pytorch-od1-fasterrcnn-resnet50-fpn`, which is a Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN). This architecture is well-suited for detecting objects of various sizes.\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "The `ModelTrainer.from_jumpstart_config()` method automatically configures:\n",
    "- The training container image\n",
    "- Default hyperparameters optimized for the model\n",
    "- Instance type for training\n",
    "\n",
    "We only need to provide:\n",
    "- The model ID\n",
    "- The S3 location of our training data\n",
    "- A base name for the training job\n",
    "\n",
    "The training process will:\n",
    "1. Load the pre-trained Faster R-CNN model\n",
    "2. Replace the final detection layer to match our 5 bird categories\n",
    "3. Fine-tune the model on our bird images\n",
    "4. Save the trained model artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select JumpStart model\n",
    "model_id = 'pytorch-od1-fasterrcnn-resnet50-fpn'\n",
    "\n",
    "js_config = JumpStartConfig(model_id=model_id)\n",
    "\n",
    "# Create input data config - ONLY training channel\n",
    "train_input = InputData(\n",
    "    channel_name='training',\n",
    "    data_source=train_s3\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = ModelTrainer.from_jumpstart_config(\n",
    "    jumpstart_config=js_config,\n",
    "    base_job_name='jumpstart-od-birds',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    input_data_config=[train_input]  # Only training channel\n",
    ")\n",
    "\n",
    "print('Trainer created')\n",
    "print(f'Hyperparameters: {trainer.hyperparameters}')\n",
    "print(f'Input data: {train_input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Check training job logs for detailed error\n",
    "training_job = trainer._latest_training_job\n",
    "print(f\"Training job name: {training_job.training_job_name}\")\n",
    "print(f\"Training job status: {training_job.training_job_status}\")\n",
    "print(f\"\\nFailure reason: {training_job.failure_reason}\")\n",
    "\n",
    "# Get CloudWatch logs\n",
    "logs_client = boto3.client('logs', region_name=sagemaker_session.boto_region_name)\n",
    "\n",
    "log_group = '/aws/sagemaker/TrainingJobs'\n",
    "\n",
    "try:\n",
    "    # List log streams for this job\n",
    "    streams = logs_client.describe_log_streams(\n",
    "        logGroupName=log_group,\n",
    "        logStreamNamePrefix=training_job.training_job_name\n",
    "    )\n",
    "    \n",
    "    if streams['logStreams']:\n",
    "        stream_name = streams['logStreams'][0]['logStreamName']\n",
    "        print(f\"\\nLog stream: {stream_name}\")\n",
    "        \n",
    "        # Get last 100 log events\n",
    "        events = logs_client.get_log_events(\n",
    "            logGroupName=log_group,\n",
    "            logStreamName=stream_name,\n",
    "            limit=100,\n",
    "            startFromHead=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Last 100 log lines ===\")\n",
    "        for event in events['events']:\n",
    "            print(event['message'])\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch logs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy the Model\n",
    "\n",
    "After training completes, we need to deploy the model to an endpoint for real-time inference.\n",
    "\n",
    "### Deployment Process\n",
    "\n",
    "Deployment involves three steps:\n",
    "\n",
    "1. **Create a Model**: Defines the model artifacts and inference container image\n",
    "2. **Create an Endpoint Configuration**: Specifies the instance type and count\n",
    "3. **Create an Endpoint**: Deploys the model to a running instance\n",
    "\n",
    "### Instance Selection\n",
    "\n",
    "We use `ml.g4dn.xlarge`, which is a GPU instance. Object detection models with deep neural networks require GPU acceleration for fast inference. CPU instances would be too slow for practical use.\n",
    "\n",
    "The endpoint will remain running until you delete it, so remember to clean up when done to avoid charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model artifacts from training job\n",
    "training_job = trainer._latest_training_job\n",
    "training_job.refresh()\n",
    "model_data = training_job.model_artifacts.s3_model_artifacts\n",
    "\n",
    "print(f'Model artifacts: {model_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core import image_uris\n",
    "\n",
    "image = image_uris.retrieve(\n",
    "    framework='pytorch',\n",
    "    region=sagemaker_session.boto_region_name,\n",
    "    image_scope='inference',\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    version='1.8.1',\n",
    "    py_version='py3',\n",
    ")\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import Model, EndpointConfig, Endpoint\n",
    "\n",
    "# Create model\n",
    "model = Model.create(\n",
    "    model_name=f'jumpstart-od-birds-{int(time.time())}',\n",
    "    execution_role_arn=role,\n",
    "    primary_container={\n",
    "        'image': image,\n",
    "        'model_data_url': model_data\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create endpoint config\n",
    "endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=f'jumpstart-od-birds-config-{int(time.time())}',\n",
    "    production_variants=[{\n",
    "        'variant_name': 'AllTraffic',\n",
    "        'model_name': model.model_name,\n",
    "        'instance_type': 'ml.g4dn.xlarge',\n",
    "        'initial_instance_count': 1\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Create endpoint\n",
    "endpoint = Endpoint.create(\n",
    "    endpoint_name=f'jumpstart-od-birds-{int(time.time())}',\n",
    "    endpoint_config_name=endpoint_config.endpoint_config_name\n",
    ")\n",
    "\n",
    "endpoint.wait_for_status('InService')\n",
    "print(f'Endpoint: {endpoint.endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model\n",
    "\n",
    "With our model deployed, we can test it by sending images and examining the predictions.\n",
    "\n",
    "### How Inference Works\n",
    "\n",
    "1. **Load an image** from the test set as raw bytes\n",
    "2. **Send to endpoint** using the SageMaker Runtime client\n",
    "3. **Parse the response** which contains detected objects\n",
    "\n",
    "### Understanding the Response\n",
    "\n",
    "The model returns three parallel arrays:\n",
    "- `normalized_boxes`: Bounding box coordinates in [0, 1] range as `[x_min, y_min, x_max, y_max]`\n",
    "- `classes`: Category IDs (1-5 for our bird species)\n",
    "- `scores`: Confidence scores (0-1) for each detection\n",
    "\n",
    "Each index represents one detected object. For example:\n",
    "- `normalized_boxes[0]` = `[0.2, 0.3, 0.8, 0.9]` (bird occupies 20-80% width, 30-90% height)\n",
    "- `classes[0]` = `1` (bird species 1)\n",
    "- `scores[0]` = `0.95` (95% confident)\n",
    "\n",
    "Let's test with a sample bird image from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection(img_file, dets, classes=[], thresh=0.5):\n",
    "    \"\"\"\n",
    "    Visualize detections with bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "    - img_file: path to image\n",
    "    - dets: detections as [[class, score, x_min, y_min, x_max, y_max], ...]\n",
    "    - classes: list of class names\n",
    "    - thresh: confidence threshold\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "    \n",
    "    img = mpimg.imread(img_file)\n",
    "    plt.imshow(img)\n",
    "    height, width = img.shape[:2]\n",
    "    colors = {}\n",
    "    num_detections = 0\n",
    "    \n",
    "    for det in dets:\n",
    "        klass, score, x0, y0, x1, y1 = det\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        num_detections += 1\n",
    "        cls_id = int(klass)\n",
    "        \n",
    "        if cls_id not in colors:\n",
    "            colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "        \n",
    "        xmin = int(x0 * width)\n",
    "        ymin = int(y0 * height)\n",
    "        xmax = int(x1 * width)\n",
    "        ymax = int(y1 * height)\n",
    "        \n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, \n",
    "                           fill=False, edgecolor=colors[cls_id], linewidth=3.5)\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        class_name = classes[cls_id - 1] if classes and cls_id <= len(classes) else str(cls_id)\n",
    "        print(f'{class_name}, {score:.3f}')\n",
    "        plt.gca().text(xmin, ymin - 2, f'{class_name} {score:.3f}',\n",
    "                      bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                      fontsize=12, color='white')\n",
    "    \n",
    "    print(f'Number of detections: {num_detections}')\n",
    "    plt.show()\n",
    "\n",
    "def predict_and_visualize(img_file, endpoint_name, thresh=0.5):\n",
    "    \"\"\"Run inference and visualize results.\"\"\"\n",
    "    runtime_client = sagemaker_session.sagemaker_runtime_client\n",
    "    \n",
    "    with open(img_file, 'rb') as f:\n",
    "        img_bytes = f.read()\n",
    "    \n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-image',\n",
    "        Body=img_bytes\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['Body'].read())\n",
    "    \n",
    "    # Convert to detection format: [class, score, x_min, y_min, x_max, y_max]\n",
    "    dets = []\n",
    "    for bbox, cls, score in zip(result['normalized_boxes'], result['classes'], result['scores']):\n",
    "        if score > 0.5:  # Only include high-confidence detections\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            dets.append([cls, score, x_min, y_min, x_max, y_max])\n",
    "    \n",
    "    class_names = [f'bird_class_{i}' for i in range(1, 6)]\n",
    "    visualize_detection(img_file, dets, class_names, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Test Images\n",
    "\n",
    "Let's download some bird images that the model hasn't seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download test images\n",
    "test_image_urls = {\n",
    "    'multi-goldfinch-1.jpg': 'https://t3.ftcdn.net/jpg/01/44/64/36/500_F_144643697_GJRUBtGc55KYSMpyg1Kucb9yJzvMQooW.jpg',\n",
    "    'hummingbird-1.jpg': 'http://res.freestockphotos.biz/pictures/17/17875-hummingbird-close-up-pv.jpg'\n",
    "}\n",
    "\n",
    "for filename, url in test_image_urls.items():\n",
    "    if not os.path.exists(filename):\n",
    "        print(f'Downloading {filename}...')\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "print('Downloaded 2 test images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(endpoint_name):\n",
    "    \"\"\"Test model with downloaded bird images.\"\"\"\n",
    "    test_images = [\n",
    "        'hummingbird-1.jpg',\n",
    "        'multi-goldfinch-1.jpg',\n",
    "    ]\n",
    "    \n",
    "    for img in test_images:\n",
    "        if os.path.exists(img):\n",
    "            print(f'\\nTesting: {img}')\n",
    "            predict_and_visualize(img, endpoint_name, thresh=0.4)\n",
    "\n",
    "# Test the model\n",
    "test_model(endpoint.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup\n",
    "\n",
    "To avoid ongoing charges, delete the endpoint when you're done testing.\n",
    "\n",
    "The endpoint runs on a GPU instance which incurs hourly charges even when not in use. Always clean up resources after experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "endpoint.delete()\n",
    "print(f'Deleted endpoint: {endpoint.endpoint_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
