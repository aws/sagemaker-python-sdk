{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to SageMaker Neural Topic Model (V3)\n",
    "\n",
    "***Unsupervised representation learning and topic extraction using Neural Topic Model***\n",
    "\n",
    "**This notebook has been migrated to SageMaker Python SDK V3**\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Data Preparation](#Data-Preparation)\n",
    "1. [Model Training](#Model-Training)\n",
    "1. [Model Hosting and Inference](#Model-Hosting-and-Inference)\n",
    "1. [Model Exploration](#Model-Exploration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction\n",
    "\n",
    "Amazon SageMaker Neural Topic Model (NTM) is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. NTM is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified upfront and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics. \n",
    "\n",
    "In this notebook, we will use the Amazon SageMaker NTM algorithm to train a model on the [20NewsGroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups) data set. This data set has been widely used as a topic modeling benchmark. \n",
    "\n",
    "The main goals of this notebook are as follows:\n",
    "\n",
    "1. learn how to obtain and store data for use in Amazon SageMaker,\n",
    "2. create an AWS SageMaker training job on a data set to produce an NTM model,\n",
    "3. use the model to perform inference with an Amazon SageMaker endpoint.\n",
    "4. explore trained model and visualized learned topics\n",
    "\n",
    "If you would like to know more please check out the [SageMaker Neural Topic Model Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Preparation\n",
    "\n",
    "The 20Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. This collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. Here, we will see what topics we can learn from this set of documents with NTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Data Set\n",
    "\n",
    "First let's define the folder to hold the data and clean the content in it which might be from previous experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "data_dir = '20_newsgroups'\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf 20_newsgroups.tar.gz\n",
    "!ls 20_newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [os.path.join(data_dir,f) for f in sorted(os.listdir(data_dir)) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "file_list = [os.path.join(d,f) for d in folders for f in os.listdir(d)]\n",
    "print('Number of documents:', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets._twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\n",
    "data = []\n",
    "for f in file_list:\n",
    "    with open(f, 'rb') as fin:\n",
    "        content = fin.read().decode('latin1')        \n",
    "        content = strip_newsgroup_header(content)\n",
    "        content = strip_newsgroup_quoting(content)\n",
    "        content = strip_newsgroup_footer(content)        \n",
    "        data.append(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## From Plain Text to Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "vocab_list = vectorizer.get_feature_names_out()\n",
    "print('vocab size:', len(vocab_list))\n",
    "print('vectors shape:', vectors.shape)\n",
    "\n",
    "idx = np.arange(vectors.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "vectors = vectors[idx]\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 25\n",
    "vectors = vectors[np.array(vectors.sum(axis=1)>threshold).reshape(-1,)]\n",
    "print('removed short docs (<{} words)'.format(threshold))        \n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * vectors.shape[0])\n",
    "\n",
    "train_vectors = vectors[:n_train, :]\n",
    "test_vectors = vectors[n_train:, :]\n",
    "\n",
    "n_test = test_vectors.shape[0]\n",
    "val_vectors = test_vectors[:n_test//2, :]\n",
    "test_vectors = test_vectors[n_test//2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_vectors.shape, test_vectors.shape, val_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Store Data on S3\n",
    "\n",
    "**V3 Migration Note**: In V3, we use CSV format instead of RecordIO Protobuf format since `sagemaker.amazon.common` module is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "\n",
    "sagemaker_session = Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '20newsgroups'\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V3 Migration**: Convert sparse matrices to CSV format and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old files from S3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_obj = s3.Bucket(bucket)\n",
    "\n",
    "for obj in bucket_obj.objects.filter(Prefix='20newsgroups/train/'):\n",
    "    obj.delete()\n",
    "for obj in bucket_obj.objects.filter(Prefix='20newsgroups/val/'):\n",
    "    obj.delete()\n",
    "\n",
    "print(\"Deleted old files. Now re-run the upload cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload CSV with extra column for NTM bug\n",
    "def upload_csv_with_extra_col(sparray, bucket, prefix, template, n_parts):\n",
    "    chunk_size = sparray.shape[0] // n_parts\n",
    "    for i in range(n_parts):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i + 1 < n_parts else sparray.shape[0]\n",
    "        \n",
    "        chunk = sparray[start:end].toarray().astype(int)\n",
    "        fname = template.format(i)\n",
    "        \n",
    "        with open(fname, 'w') as f:\n",
    "            for row in chunk:\n",
    "                # Add extra 0 column at the end for NTM bug\n",
    "                f.write(','.join(map(str, row)) + ',0\\n')\n",
    "        \n",
    "        s3_key = os.path.join(prefix, fname)\n",
    "        boto3.resource('s3').Bucket(bucket).upload_file(fname, s3_key)\n",
    "        print(f'Uploaded: s3://{bucket}/{s3_key}')\n",
    "        os.remove(fname)\n",
    "\n",
    "upload_csv_with_extra_col(train_vectors, bucket, train_prefix, 'train_part{}.csv', 8)\n",
    "upload_csv_with_extra_col(val_vectors, bucket, val_prefix, 'val_part{}.csv', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core import image_uris\n",
    "\n",
    "container = image_uris.retrieve(\n",
    "    framework='ntm',\n",
    "    region=region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.train.model_trainer import ModelTrainer\n",
    "from sagemaker.train.configs import InputData, Compute\n",
    "\n",
    "num_topics = 20\n",
    "\n",
    "print(f'Training with feature_dim={train_vectors.shape[1]}')\n",
    "trainer = ModelTrainer(\n",
    "    training_image=container,\n",
    "    role=role,\n",
    "    compute=Compute(\n",
    "        instance_count=2,\n",
    "        instance_type='ml.c4.xlarge'\n",
    "    ),\n",
    "    hyperparameters={\n",
    "        'num_topics': str(num_topics),\n",
    "        'feature_dim': '2000',\n",
    "        'mini_batch_size': '128',\n",
    "        'epochs': '100',\n",
    "        'num_patience_epochs': '5',\n",
    "        'tolerance': '0.001'\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.shapes.shapes import S3DataSource\n",
    "\n",
    "training_job = trainer.train(\n",
    "    input_data_config=[\n",
    "        InputData(\n",
    "            channel_name='train',\n",
    "            data_source=S3DataSource(\n",
    "                s3_data_type='S3Prefix',\n",
    "                s3_uri=s3_train_data,\n",
    "                s3_data_distribution_type='ShardedByS3Key'\n",
    "            ),\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        InputData(\n",
    "            channel_name='validation',\n",
    "            data_source=s3_val_data,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    ],\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job = trainer._latest_training_job\n",
    "\n",
    "print('Training job name: {}'.format(training_job.training_job_name))\n",
    "print('Training job status: {}'.format(training_job.training_job_status))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hosting and Inference\n",
    "\n",
    "**V3 Migration**: Using resource classes (`Model`, `EndpointConfig`, `Endpoint`) instead of `deploy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import Model, EndpointConfig, Endpoint\n",
    "import time\n",
    "\n",
    "model_name = f\"ntm-model-{int(time.time())}\"\n",
    "endpoint_config_name = f\"ntm-endpoint-config-{int(time.time())}\"\n",
    "endpoint_name = f\"ntm-endpoint-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import Model\n",
    "\n",
    "# Create model from training job artifacts\n",
    "model = Model.create(\n",
    "    model_name=f'ntm-model-{int(__import__(\"time\").time())}',\n",
    "    execution_role_arn=role,\n",
    "    primary_container={\n",
    "        'image': container,\n",
    "        'model_data_url': training_job.model_artifacts.s3_model_artifacts\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f'Model created: {model.model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import EndpointConfig\n",
    "\n",
    "# Create endpoint configuration\n",
    "endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=f'ntm-config-{int(__import__(\"time\").time())}',\n",
    "    production_variants=[{\n",
    "        'variant_name': 'AllTraffic',\n",
    "        'model_name': model.model_name,\n",
    "        'initial_instance_count': 1,\n",
    "        'instance_type': 'ml.m4.xlarge'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(f'Endpoint config created: {endpoint_config.endpoint_config_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import Endpoint\n",
    "\n",
    "# Create endpoint\n",
    "endpoint = Endpoint.create(\n",
    "    endpoint_name=f'ntm-endpoint-{int(__import__(\"time\").time())}',\n",
    "    endpoint_config_name=endpoint_config.endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f'Endpoint created: {endpoint.endpoint_name}')\n",
    "endpoint.wait_for_status('InService')\n",
    "print('Endpoint is ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(endpoint.endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Serialization/Deserialization\n",
    "\n",
    "**V3 Migration**: Using `endpoint.invoke()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def np2csv(arr):\n",
    "    csv = '\\n'.join([','.join([str(x) for x in row]) for row in arr])\n",
    "    return csv\n",
    "\n",
    "test_data = np.array(test_vectors.todense())\n",
    "payload = np2csv(test_data[:5])\n",
    "\n",
    "response = endpoint.invoke(\n",
    "    body=payload,\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "results = json.loads(response.body.read().decode())\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs = 12\n",
    "df=pd.DataFrame(predictions.T)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint\n",
    "\n",
    "**V3 Migration**: Using `endpoint.delete()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Exploration\n",
    "\n",
    "The trained NTM model contains learned topic representations. We can download and explore the model artifacts to understand the topics discovered in the 20 newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job reference\n",
    "training_job = trainer._latest_training_job\n",
    "print(f\"Training job: {training_job.training_job_name}\")\n",
    "print(f\"Status: {training_job.training_job_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distributions from endpoint\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Sample diverse documents from test set\n",
    "test_data = np.array(test_vectors.todense())\n",
    "sample_size = min(500, test_data.shape[0])\n",
    "sample_indices = np.linspace(0, test_data.shape[0]-1, sample_size, dtype=int)\n",
    "test_sample = test_data[sample_indices]\n",
    "\n",
    "print(f\"Using {sample_size} diverse samples\")\n",
    "\n",
    "# Get predictions\n",
    "payload = np2csv(test_sample)\n",
    "response = endpoint.invoke(body=payload, content_type=\"text/csv\")\n",
    "results = json.loads(response.body.read().decode())\n",
    "\n",
    "# Extract topic distributions\n",
    "topic_distributions = np.array([pred[\"topic_weights\"] for pred in results[\"predictions\"]])\n",
    "print(f\"Topic distributions shape: {topic_distributions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract distinctive words per topic\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "custom_stops = set([\n",
    "    \"don\", \"just\", \"think\", \"people\", \"like\", \"know\", \"time\", \"does\", \"said\",\n",
    "    \"did\", \"way\", \"say\", \"good\", \"right\", \"ve\", \"ll\", \"didn\", \"doesn\", \"isn\",\n",
    "    \"wasn\", \"aren\", \"god\", \"religion\", \"believe\", \"point\", \"things\", \"thing\",\n",
    "    \"make\", \"want\", \"going\", \"really\", \"question\", \"post\", \"better\", \"claim\"\n",
    "])\n",
    "all_stops = ENGLISH_STOP_WORDS.union(custom_stops)\n",
    "\n",
    "def get_distinctive_words(topic_idx, n_words=20):\n",
    "    topic_strengths = topic_distributions[:, topic_idx]\n",
    "    high_threshold = np.percentile(topic_strengths, 85)\n",
    "    high_mask = topic_strengths > high_threshold\n",
    "    low_threshold = np.percentile(topic_strengths, 50)\n",
    "    low_mask = topic_strengths < low_threshold\n",
    "    \n",
    "    if high_mask.sum() < 5 or low_mask.sum() < 5:\n",
    "        return []\n",
    "    \n",
    "    high_docs = test_sample[high_mask]\n",
    "    low_docs = test_sample[low_mask]\n",
    "    high_freq = (high_docs > 0).sum(axis=0) / high_mask.sum()\n",
    "    low_freq = (low_docs > 0).sum(axis=0) / low_mask.sum()\n",
    "    diff = high_freq - low_freq\n",
    "    \n",
    "    filtered_words = []\n",
    "    for idx in np.argsort(diff)[::-1]:\n",
    "        word = vocab_list[idx]\n",
    "        if word.lower() not in all_stops and len(word) > 2 and diff[idx] > 0:\n",
    "            filtered_words.append((word, diff[idx]))\n",
    "            if len(filtered_words) >= n_words:\n",
    "                break\n",
    "    return filtered_words\n",
    "\n",
    "print(\"Distinctive Topic Words:\")\n",
    "print(\"=\"*60)\n",
    "for topic_idx in range(min(5, topic_distributions.shape[1])):\n",
    "    words = get_distinctive_words(topic_idx, 10)\n",
    "    if words:\n",
    "        print(f\"\\nTopic {topic_idx}: \" + \", \".join([word for word, _ in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avg_topic_strength = topic_distributions.mean(axis=0)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(avg_topic_strength)), avg_topic_strength)\n",
    "plt.xlabel(\"Topic Index\")\n",
    "plt.ylabel(\"Average Strength\")\n",
    "plt.title(\"Average Topic Strength Across Documents\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for all topics\n",
    "!pip install wordcloud -q\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "fig, axes = plt.subplots(5, 4, figsize=(20, 25))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < topic_distributions.shape[1]:\n",
    "        words = get_distinctive_words(idx, 50)\n",
    "        if words:\n",
    "            word_freq = {word: score for word, score in words}\n",
    "            wc = WordCloud(width=600, height=400,\n",
    "                          background_color=\"white\",\n",
    "                          colormap=\"tab20\").generate_from_frequencies(word_freq)\n",
    "            ax.imshow(wc, interpolation=\"bilinear\")\n",
    "        ax.set_title(f\"Topic {idx}\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.axis(\"off\")\n",
    "    else:\n",
    "        ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ntm_topics.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Generated word clouds for {topic_distributions.shape[1]} topics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
