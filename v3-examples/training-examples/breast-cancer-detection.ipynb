{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42b5e80b-ad1d-4335-a1f7-10a91127e3dc"
    }
   },
   "source": [
    "# Breast Cancer Prediction (SageMaker V3)\n",
    "_**Predict Breast Cancer using SageMaker's Linear-Learner with features derived from images of Breast Mass**_\n",
    "\n",
    "---\n",
    "\n",
    "This notebook has been migrated to use SageMaker Python SDK V3 interfaces.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "1. [Predict](#Predict)\n",
    "1. [Extensions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "This notebook illustrates how one can use SageMaker's algorithms for solving applications which require `linear models` for prediction. For this illustration, we have taken an example for breast cancer prediction using UCI'S breast cancer diagnostic data set available at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29. The data set is also available on Kaggle at https://www.kaggle.com/uciml/breast-cancer-wisconsin-data. The purpose here is to use this data set to build a predictve model of whether a breast mass image indicates benign or malignant tumor. The data set will be used to illustrate\n",
    "\n",
    "* Basic setup for using SageMaker V3.\n",
    "* converting datasets to protobuf format used by the Amazon SageMaker algorithms and uploading to S3. \n",
    "* Training SageMaker's linear learner on the data set using ModelTrainer.\n",
    "* Hosting the trained model using V3 resources.\n",
    "* Scoring using the trained model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The SageMaker role arn used to give learning and hosting access to your data.\n",
    "* The S3 bucket that you want to use for training and storing model objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "nbpresent": {
     "id": "6427e831-8f89-45c0-b150-0b134397d79a"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "# V3 imports\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "from sagemaker.core import image_uris\n",
    "from sagemaker.train.model_trainer import ModelTrainer\n",
    "from sagemaker.train.configs import InputData, Compute\n",
    "from sagemaker.core.resources import Model, EndpointConfig, Endpoint\n",
    "\n",
    "# Initialize V3 session\n",
    "sagemaker_session = Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'sagemaker/DEMO-breast-cancer-prediction-v3' # place to upload training files within the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b2548d66-6f8f-426f-9cda-7a3cd1459abd"
    }
   },
   "source": [
    "Now we'll import the Python libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bb88eea9-27f3-4e47-9133-663911ea09a9"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "142777ae-c072-448e-b941-72bc75735d01"
    }
   },
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "Data Source: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
    "        https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "\n",
    "Let's download the data and save it in the local folder with the name data.csv and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f8976dad-6897-4c7e-8c95-ae2f53070ef5"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header = None)\n",
    "\n",
    "# specify columns extracted from wbdc.names\n",
    "data.columns = [\"id\",\"diagnosis\",\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\n",
    "                \"compactness_mean\",\"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\n",
    "                \"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\n",
    "                \"concave points_se\",\"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\n",
    "                \"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\n",
    "                \"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"] \n",
    "\n",
    "# save the data\n",
    "data.to_csv(\"data.csv\", sep=',', index=False)\n",
    "\n",
    "# print the shape of the data file\n",
    "print(data.shape)\n",
    "\n",
    "# show the top few rows\n",
    "display(data.head())\n",
    "\n",
    "# describe the data object\n",
    "display(data.describe())\n",
    "\n",
    "# we will also summarize the categorical field diganosis \n",
    "display(data.diagnosis.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key observations:\n",
    "* Data has 569 observations and 32 columns.\n",
    "* First field is 'id'.\n",
    "* Second field, 'diagnosis', is an indicator of the actual diagnosis ('M' = Malignant; 'B' = Benign).\n",
    "* There are 30 other numeric features available for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features and Labels\n",
    "#### Split the data into 80% training, 10% validation and 10% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_split = np.random.rand(len(data))\n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "test_list = rand_split >= 0.9\n",
    "\n",
    "data_train = data[train_list]\n",
    "data_val = data[val_list]\n",
    "data_test = data[test_list]\n",
    "\n",
    "train_y = ((data_train.iloc[:,1] == 'M') +0).to_numpy();\n",
    "train_X = data_train.iloc[:,2:].to_numpy();\n",
    "\n",
    "val_y = ((data_val.iloc[:,1] == 'M') +0).to_numpy();\n",
    "val_X = data_val.iloc[:,2:].to_numpy();\n",
    "\n",
    "test_y = ((data_test.iloc[:,1] == 'M') +0).to_numpy();\n",
    "test_X = data_test.iloc[:,2:].to_numpy();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ff9d10f9-b611-423b-80da-6dcdafd1c8b9"
    }
   },
   "source": [
    "Now, we'll convert the datasets to CSV format and upload to S3. Linear Learner expects the label in the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cd8e3431-79d9-40b6-91d1-d67cd61894e7"
    }
   },
   "outputs": [],
   "source": [
    "# Training data - label in first column\n",
    "train_df = pd.DataFrame(train_X)\n",
    "train_df.insert(0, 'label', train_y)\n",
    "train_file = 'linear_train.csv'\n",
    "train_df.to_csv(train_file, header=False, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'train', train_file)\n",
    ").upload_file(train_file)\n",
    "\n",
    "train_s3_uri = f\"s3://{bucket}/{prefix}/train/{train_file}\"\n",
    "print(f\"Training data uploaded to: {train_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "71cbcebd-a2a5-419e-8e50-b2bc0909f564"
    }
   },
   "source": [
    "Next we'll convert and upload the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bd113b8e-adc1-4091-a26f-a426149fe604"
    }
   },
   "outputs": [],
   "source": [
    "# Validation data - label in first column\n",
    "val_df = pd.DataFrame(val_X)\n",
    "val_df.insert(0, 'label', val_y)\n",
    "validation_file = 'linear_validation.csv'\n",
    "val_df.to_csv(validation_file, header=False, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'validation', validation_file)\n",
    ").upload_file(validation_file)\n",
    "\n",
    "validation_s3_uri = f\"s3://{bucket}/{prefix}/validation/{validation_file}\"\n",
    "print(f\"Validation data uploaded to: {validation_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f3b125ad-a2d5-464c-8cfa-bd203034eee4"
    }
   },
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Now we can begin to specify our linear model using SageMaker V3's ModelTrainer.  Amazon SageMaker's Linear Learner actually fits many models in parallel, each with slightly different hyperparameters, and then returns the one with the best fit.  This functionality is automatically enabled.  We can influence this using parameters like:\n",
    "\n",
    "- `num_models` to increase to total number of models run.  The specified parameters will always be one of those models, but the algorithm also chooses models with nearby parameter values in order to find a solution nearby that may be more optimal.  In this case, we're going to use the max of 32.\n",
    "- `loss` which controls how we penalize mistakes in our model estimates.  For this case, let's use absolute loss as we haven't spent much time cleaning the data, and absolute loss will be less sensitive to outliers.\n",
    "- `wd` or `l1` which control regularization.  Regularization can prevent model overfitting by preventing our estimates from becoming too finely tuned to the training data, which can actually hurt generalizability.  In this case, we'll leave these parameters as their default \"auto\" though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get container image for linear-learner using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3: Use image_uris.retrieve instead of get_image_uri\n",
    "container = image_uris.retrieve(\n",
    "    framework='linear-learner',\n",
    "    region=region\n",
    ")\n",
    "print(f\"Using container: {container}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model using ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "397fb60a-c48b-453f-88ea-4d832b70c919"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# V3: Use ModelTrainer instead of boto3 create_training_job\n",
    "trainer = ModelTrainer(\n",
    "    training_image=container,\n",
    "    role=role,\n",
    "    compute=Compute(\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.c4.2xlarge\",\n",
    "        volume_size_in_gb=10\n",
    "    ),\n",
    "    hyperparameters={\n",
    "        \"feature_dim\": \"30\",\n",
    "        \"mini_batch_size\": \"100\",\n",
    "        \"predictor_type\": \"regressor\",\n",
    "        \"epochs\": \"10\",\n",
    "        \"num_models\": \"32\",\n",
    "        \"loss\": \"absolute_loss\"\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "training_job = trainer.train(\n",
    "    input_data_config=[\n",
    "        InputData(\n",
    "            channel_name=\"train\",\n",
    "            data_source=train_s3_uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        InputData(\n",
    "            channel_name=\"validation\",\n",
    "            data_source=validation_s3_uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    ],\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "# Get the training job from the trainer\n",
    "training_job = trainer._latest_training_job\n",
    "print(f\"Training job completed: {training_job.training_job_name}\")\n",
    "print(f\"Model artifacts: {training_job.model_artifacts.s3_model_artifacts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2adcc348-9ab5-4a8a-8139-d0ecd740208a"
    }
   },
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Now that we've trained the linear algorithm on our data, let's setup a model which can later be hosted using V3 resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c88fb868-01d2-4991-8953-28814c022bdc"
    }
   },
   "outputs": [],
   "source": [
    "# V3: Create Model using resources\n",
    "model_name = f\"breast-cancer-model-{time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())}\"\n",
    "\n",
    "model = Model.create(\n",
    "    model_name=model_name,\n",
    "    execution_role_arn=role,\n",
    "    primary_container={\n",
    "        'image': container,\n",
    "        'model_data_url': training_job.model_artifacts.s3_model_artifacts\n",
    "    },\n",
    "    session=sagemaker_session.boto_session\n",
    ")\n",
    "\n",
    "print(f\"Model created: {model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3: Create EndpointConfig\n",
    "endpoint_config_name = f\"breast-cancer-config-{time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())}\"\n",
    "\n",
    "endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=endpoint_config_name,\n",
    "    production_variants=[{\n",
    "        'variant_name': 'AllTraffic',\n",
    "        'model_name': model.model_name,\n",
    "        'instance_type': 'ml.m4.xlarge',\n",
    "        'initial_instance_count': 1\n",
    "    }],\n",
    "    session=sagemaker_session.boto_session\n",
    ")\n",
    "\n",
    "print(f\"Endpoint config created: {endpoint_config.endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# V3: Create Endpoint\n",
    "endpoint_name = f\"breast-cancer-endpoint-{time.strftime('%Y%m%d%H%M', time.gmtime())}\"\n",
    "\n",
    "endpoint = Endpoint.create(\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_config_name=endpoint_config.endpoint_config_name,\n",
    "    session=sagemaker_session.boto_session\n",
    ")\n",
    "\n",
    "print(f\"Endpoint created: {endpoint.endpoint_name}\")\n",
    "print(\"Waiting for endpoint to be in service...\")\n",
    "\n",
    "# Wait for endpoint to be ready\n",
    "endpoint.wait_for_status('InService')\n",
    "\n",
    "print(f\"Endpoint is ready: {endpoint.endpoint_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "### Predict on Test Data\n",
    "\n",
    "Now that we have our hosted endpoint, we can generate statistical predictions from it.  Let's predict on our test dataset to understand how accurate our model is.\n",
    "\n",
    "There are many metrics to measure classification accuracy.  Common examples include include:\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 measure\n",
    "- Area under the ROC curve - AUC\n",
    "- Total Classification Accuracy \n",
    "- Mean Absolute Error\n",
    "\n",
    "For our example, we'll keep things simple and use total classification accuracy as our metric of choice. We will also evaluate  Mean Absolute  Error (MAE) as the linear-learner has been optimized using this metric, not necessarily because it is a relevant metric from an application point of view. We'll compare the performance of the linear-learner against a naive benchmark prediction which uses majority class observed in the training data set for prediction on the test data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert an array to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll invoke the endpoint to get predictions using V3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3: Use endpoint.invoke instead of runtime.sagemaker\n",
    "payload = np2csv(test_X)\n",
    "\n",
    "response = endpoint.invoke(\n",
    "    body=payload,\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "result = json.loads(response.body.read().decode())\n",
    "test_pred = np.array([r['score'] for r in result['predictions']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare linear learner based mean absolute prediction errors from a baseline prediction which uses majority class to predict every instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae_linear = np.mean(np.abs(test_y - test_pred))\n",
    "test_mae_baseline = np.mean(np.abs(test_y - np.median(train_y))) ## training median as baseline predictor\n",
    "\n",
    "print(\"Test MAE Baseline :\", round(test_mae_baseline, 3))\n",
    "print(\"Test MAE Linear:\", round(test_mae_linear,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare predictive accuracy using a classification threshold of 0.5 for the predicted and compare against the majority class prediction from training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_class = (test_pred > 0.5)+0;\n",
    "test_pred_baseline = np.repeat(np.median(train_y), len(test_y))\n",
    "\n",
    "prediction_accuracy = np.mean((test_y == test_pred_class))*100\n",
    "baseline_accuracy = np.mean((test_y == test_pred_baseline))*100\n",
    "\n",
    "print(\"Prediction Accuracy:\", round(prediction_accuracy,1), \"%\")\n",
    "print(\"Baseline Accuracy:\", round(baseline_accuracy,1), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Run the cell below to delete endpoint once you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3: Use endpoint.delete()\n",
    "endpoint.delete()\n",
    "print(f\"Endpoint {endpoint_name} deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extensions\n",
    "\n",
    "- Our linear model does a good job of predicting breast cancer and has an overall accuracy of close to 92%. We can re-run the model with different values of the hyper-parameters, loss functions etc and see if we get improved prediction. Re-running the model with further tweaks to these hyperparameters may provide more accurate out-of-sample predictions.\n",
    "- We also did not do much feature engineering. We can create additional features by considering cross-product/intreaction of multiple features, squaring or raising higher powers of the features to induce non-linear effects, etc. If we expand the features using non-linear terms and interactions, we can then tweak the regulaization parameter to optimize the expanded model and hence generate improved forecasts.\n",
    "- As a further extension, we can use many of non-linear models available through SageMaker such as XGBoost, MXNet etc.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the License). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the license file accompanying this file. This file is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
