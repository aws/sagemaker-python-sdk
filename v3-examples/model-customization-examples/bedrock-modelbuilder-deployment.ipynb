{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock ModelBuilder Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS credentials and region\n",
    "#! ada credentials update --provider=isengard --account=<> --role=Admin --profile=default --once\n",
    "#! aws configure set region us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from sagemaker.core.resources import TrainingJob\n",
    "from sagemaker.serve.bedrock_model_builder import BedrockModelBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TRAINING_JOB_NAME = 'meta-textgeneration-llama-3-2-1b-instruct-sft-20251123162832'\n",
    "ROLE_ARN = \"arn:aws:iam::<>:role/Admin\"\n",
    "REGION = 'us-west-2'\n",
    "BUCKET = 'open-models-testing-pdx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get training job and prepare model path\n",
    "training_job = TrainingJob.get(training_job_name=TRAINING_JOB_NAME)\n",
    "print(f\"Training job status: {training_job.training_job_status}\")\n",
    "\n",
    "# Use the hf_merged directory which has complete HuggingFace format\n",
    "base_s3_path = training_job.model_artifacts.s3_model_artifacts\n",
    "hf_model_path = base_s3_path.rstrip('/') + '/checkpoints/hf_merged/'\n",
    "print(f\"Using HF model path: {hf_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Verify required files exist\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "\n",
    "required_files = ['config.json', 'tokenizer.json', 'tokenizer_config.json', 'model.safetensors']\n",
    "model_prefix = hf_model_path.replace(f's3://{BUCKET}/', '')\n",
    "\n",
    "print(\"Checking required files:\")\n",
    "for file in required_files:\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=BUCKET, Key=model_prefix + file)\n",
    "        print(f\"‚úÖ {file}\")\n",
    "    except:\n",
    "        print(f\"‚ùå {file} - MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create missing tokenizer files if needed\n",
    "def ensure_tokenizer_files():\n",
    "    # Create added_tokens.json (usually empty for Llama)\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=BUCKET, Key=model_prefix + 'added_tokens.json')\n",
    "        print(\"‚úÖ added_tokens.json exists\")\n",
    "    except:\n",
    "        s3_client.put_object(\n",
    "            Bucket=BUCKET,\n",
    "            Key=model_prefix + 'added_tokens.json',\n",
    "            Body=json.dumps({}),\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        print(\"‚úÖ Created added_tokens.json\")\n",
    "\n",
    "ensure_tokenizer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check what's actually in the S3 bucket\n",
    "print(\"Checking S3 structure...\")\n",
    "base_prefix = base_s3_path.replace(f's3://{BUCKET}/', '')\n",
    "print(f\"Base prefix: {base_prefix}\")\n",
    "\n",
    "# List files to see the actual structure\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=BUCKET,\n",
    "    Prefix=base_prefix,\n",
    "    Delimiter='/'\n",
    ")\n",
    "\n",
    "print(\"Contents:\")\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents'][:10]:  # Show first 10 files\n",
    "        print(f\"  {obj['Key']}\")\n",
    "\n",
    "# Check specifically for hf_merged directory\n",
    "hf_merged_prefix = base_prefix.rstrip('/') + '/checkpoints/hf_merged/'\n",
    "print(f\"\\nChecking hf_merged path: {hf_merged_prefix}\")\n",
    "\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=BUCKET, Prefix=hf_merged_prefix)\n",
    "    if 'Contents' in response:\n",
    "        print(\"Files in hf_merged:\")\n",
    "        for obj in response['Contents']:\n",
    "            file_name = obj['Key'].replace(hf_merged_prefix, '')\n",
    "            print(f\"  {file_name}\")\n",
    "            \n",
    "        # Now copy with correct paths\n",
    "        for obj in response['Contents']:\n",
    "            source_key = obj['Key']\n",
    "            file_name = source_key.replace(hf_merged_prefix, '')\n",
    "            dest_key = base_prefix.rstrip('/') + '/' + file_name\n",
    "            \n",
    "            try:\n",
    "                s3_client.copy_object(\n",
    "                    Bucket=BUCKET,\n",
    "                    CopySource={'Bucket': BUCKET, 'Key': source_key},\n",
    "                    Key=dest_key\n",
    "                )\n",
    "                print(f\"‚úÖ Copied {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to copy {file_name}: {e}\")\n",
    "    else:\n",
    "        print(\"No files found in hf_merged directory\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Bedrock model builder and deploy\n",
    "job_name = f\"bedrock-import-{random.randint(1000, 9999)}-{int(time.time())}\"\n",
    "print(f\"Job name: {job_name}\")\n",
    "\n",
    "# Create builder with correct model path\n",
    "bedrock_builder = BedrockModelBuilder(\n",
    "    model=training_job\n",
    ")\n",
    "\n",
    "# Deploy to Bedrock\n",
    "deployment_result = bedrock_builder.deploy(\n",
    "    job_name=job_name,\n",
    "    imported_model_name=job_name,\n",
    "    role_arn=ROLE_ARN\n",
    ")\n",
    "\n",
    "job_arn = deployment_result['jobArn']\n",
    "print(f\"Import job started: {job_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Wait for import to complete\n",
    "bedrock_client = boto3.client('bedrock', region_name=REGION)\n",
    "\n",
    "print(\"Waiting for import to complete...\")\n",
    "while True:\n",
    "    response = bedrock_client.get_model_import_job(jobIdentifier=job_arn)\n",
    "    status = response['status']\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status == 'Completed':\n",
    "        imported_model_arn = response['importedModelArn']\n",
    "        print(f\"‚úÖ Import completed!\")\n",
    "        print(f\"Model ARN: {imported_model_arn}\")\n",
    "        break\n",
    "    elif status in ['Failed', 'Stopped']:\n",
    "        print(f\"‚ùå Import failed: {status}\")\n",
    "        if 'failureMessage' in response:\n",
    "            print(f\"Error: {response['failureMessage']}\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Test inference with correct format\n",
    "if 'imported_model_arn' in locals():\n",
    "    bedrock_runtime = boto3.client('bedrock-runtime', region_name=REGION)\n",
    "    \n",
    "    # Try ChatCompletion format (OpenAI-style)\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=imported_model_arn,\n",
    "            body=json.dumps({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "                ],\n",
    "                \"max_tokens\": 100,\n",
    "                \"temperature\": 0.7\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['body'].read().decode())\n",
    "        print(\"\\nüéâ Inference successful (ChatCompletion format)!\")\n",
    "        print(f\"Response: {result}\")\n",
    "        \n",
    "    except Exception as e1:\n",
    "        print(f\"ChatCompletion failed: {e1}\")\n",
    "        \n",
    "        # Try BedrockMetaCompletion format\n",
    "        try:\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId=imported_model_arn,\n",
    "                body=json.dumps({\n",
    "                    \"prompt\": \"What is the capital of France?\",\n",
    "                    \"max_gen_len\": 100,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"top_p\": 0.9\n",
    "                })\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['body'].read().decode())\n",
    "            print(\"\\nüéâ Inference successful (BedrockMeta format)!\")\n",
    "            print(f\"Response: {result}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"BedrockMeta failed: {e2}\")\n",
    "            print(\"‚ùå Both formats failed. Check model documentation for correct format.\")\n",
    "else:\n",
    "    print(\"‚ùå Import failed, cannot test inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: List all imported models\n",
    "models = bedrock_client.list_imported_models()\n",
    "print(\"\\nAll imported models:\")\n",
    "for model in models['modelSummaries']:\n",
    "    print(f\"- {model['modelName']}: {model['modelArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T01:09:24.972978Z",
     "start_time": "2025-11-27T01:09:18.454635Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sagemaker.core.resources import TrainingJob\n",
    "from sagemaker.serve.bedrock_model_builder import BedrockModelBuilder\n",
    "\n",
    "training_job = TrainingJob.get(training_job_name=\"kssharda-sft-lora-lite-2-ui-run-2bn3c-<>8\",\n",
    "                               region=\"us-east-1\")\n",
    "pprint(training_job.model_artifacts.s3_model_artifacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T01:09:30.542741Z",
     "start_time": "2025-11-27T01:09:28.668735Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bedrock_model_builder = BedrockModelBuilder(\n",
    "    model = training_job\n",
    ")\n",
    "\n",
    "bedrock_model_builder.deploy(job_name = \"nargokul-26-01\",\n",
    "                             custom_model_name = \"nargokul-26-01\",\n",
    "                             role_arn=\"arn:aws:iam::<>:role/Admin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.ai_registry.dataset import DataSet\n",
    "\n",
    "dataset = DataSet.get(name=\"arn:aws:sagemaker:us-east-1:<>:hub-content/MDG6N5CA58D0IJMC1OPJOPIKOS2VPPLP0AM6UBOT9D73B8A34HTG/DataSet/nova-2-0-sft-dataset/1.0.0\")\n",
    "\n",
    "pprint(dataset.__dict__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
