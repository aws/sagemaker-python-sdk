{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Clarify E2E Test\n",
    "\n",
    "Simple end-to-end test for the Clarify utils implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import boto3\n",
    "\n",
    "# Add the clarify utils to path\n",
    "# sys.path.insert(0, '/Users/mollyhe/Documents/SageMaker/sagemaker-python-sdk-staging-molly/sagemaker_utils/src')\n",
    "\n",
    "from sagemaker.core.clarify import (\n",
    "    SageMakerClarifyProcessor,\n",
    "    DataConfig,\n",
    "    BiasConfig,\n",
    "    ModelConfig,\n",
    "    SHAPConfig\n",
    ")\n",
    "from sagemaker.core.helper.session_helper import Session,get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add a sensitive feature (simulating gender: 0=female, 1=male)\n",
    "sensitive_feature = np.random.binomial(1, 0.4, size=X.shape[0])\n",
    "X = np.column_stack([X, sensitive_feature])\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'feature_{i}' for i in range(10)] + ['gender']\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution: {df['target'].value_counts()}\")\n",
    "print(f\"Gender distribution: {df['gender'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Model accuracy: {model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 paths\n",
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'clarify-test'\n",
    "\n",
    "# Save test data (without target for inference)\n",
    "test_data = X_test.copy()\n",
    "test_data['target'] = y_test\n",
    "test_data.to_csv('/tmp/test_data.csv', index=False)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, '/tmp/model.joblib')\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file('/tmp/test_data.csv', bucket, f'{prefix}/data/test_data.csv')\n",
    "s3_client.upload_file('/tmp/model.joblib', bucket, f'{prefix}/model/model.joblib')\n",
    "\n",
    "data_uri = f's3://{bucket}/{prefix}/data/test_data.csv'\n",
    "output_uri = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "print(f\"Data uploaded to: {data_uri}\")\n",
    "print(f\"Output will be saved to: {output_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Clarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "data_config = DataConfig(\n",
    "    s3_data_input_path=data_uri,\n",
    "    s3_output_path=output_uri,\n",
    "    label='target',\n",
    "    headers=list(test_data.columns),\n",
    "    dataset_type='text/csv'\n",
    ")\n",
    "\n",
    "# Bias configuration\n",
    "bias_config = BiasConfig(\n",
    "    label_values_or_threshold=[1],  # Positive class\n",
    "    facet_name='gender',\n",
    "    facet_values_or_threshold=[1]   # Male as sensitive group\n",
    ")\n",
    "\n",
    "# SHAP configuration\n",
    "shap_config = SHAPConfig(\n",
    "    baseline=None,  # Auto-generate baseline\n",
    "    num_samples=10,  # Small number for quick test\n",
    "    agg_method='mean_abs'\n",
    ")\n",
    "\n",
    "print(\"Configurations created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Clarify Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Clarify processor\n",
    "clarify_processor = SageMakerClarifyProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "print(f\"Clarify processor created with role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Pre-training Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pre-training bias analysis (no model needed)\n",
    "try:\n",
    "    clarify_processor.run_pre_training_bias(\n",
    "        data_config=data_config,\n",
    "        data_bias_config=bias_config,\n",
    "        methods=['CI', 'DPL'],  # Class Imbalance and Difference in Positive Proportions\n",
    "        wait=False,  # Don't wait for completion in test\n",
    "        logs=False\n",
    "    )\n",
    "    print(\"✅ Pre-training bias analysis job submitted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pre-training bias analysis failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can go to SageMaker AI console -> Processing jobs and check the job status\n",
    "# Or you can run the below command\n",
    "# Note that it takes ~5min for the job to be complete\n",
    "\n",
    "response = session.sagemaker_client.describe_processing_job(ProcessingJobName='Clarify-Pretraining-Bias-2025-11-09-02-39-36-699')\n",
    "print(f\"Status: {response['ProcessingJobStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Configuration Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the internal config generation\n",
    "from sagemaker.core.clarify import _AnalysisConfigGenerator\n",
    "\n",
    "try:\n",
    "    # Generate bias config\n",
    "    bias_analysis_config = _AnalysisConfigGenerator.bias_pre_training(\n",
    "        data_config=data_config,\n",
    "        bias_config=bias_config,\n",
    "        methods=['CI', 'DPL']\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Bias analysis config generated successfully\")\n",
    "    print(f\"Config keys: {list(bias_analysis_config.keys())}\")\n",
    "    \n",
    "    # Validate config structure\n",
    "    required_keys = ['dataset_type', 'label_values_or_threshold', 'facet', 'methods']\n",
    "    missing_keys = [key for key in required_keys if key not in bias_analysis_config]\n",
    "    \n",
    "    if missing_keys:\n",
    "        print(f\"❌ Missing required keys: {missing_keys}\")\n",
    "    else:\n",
    "        print(\"✅ All required keys present in config\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Config generation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test schema validation\n",
    "from sagemaker.core.clarify import ANALYSIS_CONFIG_SCHEMA_V1_0\n",
    "\n",
    "try:\n",
    "    # Validate the generated config\n",
    "    ANALYSIS_CONFIG_SCHEMA_V1_0.validate(bias_analysis_config)\n",
    "    print(\"✅ Schema validation passed\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Schema validation failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
